#+TITLE: Report of TOP Project
#+AUTHOR: Bouton Nicolas
#+DATE: April 2021

* Debugging
** Multiple definition

   For beginnig the code have a link problem at the compile time because we
   define 3 variables in *lbm_phys.h* and we redefine them in *lbm_phys.h*. Then
   we have a multiple definition.

   #+BEGIN_SRC shell
[148] [sholde@ground simu_simple_LBM] (master) (6m32s) $ make
mpicc -Wall -g -c -o main.o main.c
mpicc -Wall -g -c -o lbm_phys.o lbm_phys.c
mpicc -Wall -g -c -o lbm_init.o lbm_init.c
mpicc -Wall -g -c -o lbm_struct.o lbm_struct.c
mpicc -Wall -g -c -o lbm_comm.o lbm_comm.c
mpicc -Wall -g -c -o lbm_config.o lbm_config.c
mpicc -Wall -g -o lbm main.o lbm_phys.o lbm_init.o lbm_struct.o lbm_comm.o lbm_config.o -lm
/usr/bin/ld : lbm_phys.o:/home/sholde/dev/master/M1/S2/hm/top_project/simu_simple_LBM/lbm_phys.h:9 : définitions multiples de « opposite_of »; main.o:/home/sholde/dev/master/M1/S2/hm/top_project/simu_simple_LBM/lbm_phys.h:9 : défini pour la première fois ici
/usr/bin/ld : lbm_phys.o:/home/sholde/dev/master/M1/S2/hm/top_project/simu_simple_LBM/lbm_phys.h:10 : définitions multiples de « equil_weight »; main.o:/home/sholde/dev/master/M1/S2/hm/top_project/simu_simple_LBM/lbm_phys.h:10 : défini pour la première fois ici
/usr/bin/ld : lbm_phys.o:/home/sholde/dev/master/M1/S2/hm/top_project/simu_simple_LBM/lbm_phys.h:11 : définitions multiples de « direction_matrix »; main.o:/home/sholde/dev/master/M1/S2/hm/top_project/simu_simple_LBM/lbm_phys.h:11 : défini pour la première fois ici
/usr/bin/ld : lbm_init.o:/home/sholde/dev/master/M1/S2/hm/top_project/simu_simple_LBM/lbm_phys.h:9 : définitions multiples de « opposite_of »; main.o:/home/sholde/dev/master/M1/S2/hm/top_project/simu_simple_LBM/lbm_phys.h:9 : défini pour la première fois ici
/usr/bin/ld : lbm_init.o:/home/sholde/dev/master/M1/S2/hm/top_project/simu_simple_LBM/lbm_phys.h:10 : définitions multiples de « equil_weight »; main.o:/home/sholde/dev/master/M1/S2/hm/top_project/simu_simple_LBM/lbm_phys.h:10 : défini pour la première fois ici
/usr/bin/ld : lbm_init.o:/home/sholde/dev/master/M1/S2/hm/top_project/simu_simple_LBM/lbm_phys.h:11 : définitions multiples de « direction_matrix »; main.o:/home/sholde/dev/master/M1/S2/hm/top_project/simu_simple_LBM/lbm_phys.h:11 : défini pour la première fois ici
collect2: erreur: ld a retourné le statut de sortie 1
make: *** [Makefile:24 : lbm] Erreur 1
   #+END_SRC

   To fix the problem we just need to mark these variable extern in header file
   to indicate they are define in another file.

   #+BEGIN_SRC c
/********************** CONSTS **********************/
extern const int opposite_of[DIRECTIONS];
extern const double equil_weight[DIRECTIONS];
extern const Vector direction_matrix[DIRECTIONS];
   #+END_SRC

** Program doesn't give back and abort sometimes

   When we execute the code now, with the command ~mpirun -np 512
   --oversubscribe ./lbm~, thr program doesn't give back.

   I decide to run *gdb* and try to identify where we block. And *gdb* say me
   that I have a *segfault* in the function
   *setup_init_state_global_poiseuille_profile* in file *lbm_init.c* at line 85.

   This line contain the result of a call fonction. But as the *segfault* is in
   this function, it is not in the call funtion. Therefore it is the affectaion
   the error and either the array are not allocate or we go out of the limit of
   our array.

   When I reading the code I saw in the init function that malloc call was
   commented. I decomment it and the error is corrected.

** Program doesn't give back and abort sometimes 2

   I re-run *gdb* and it says me that I have a *segfault* when I call the
   *libc*. It is true because an allocation via *malloc* failed (printed in
   stdout).
   
   #+BEGIN_SRC shell
malloc: Permission denied
   #+END_SRC

   The problem was that I don't realize we set address of the last problem to
   *NULL* after we allocate it. I deleted the line.

   #+BEGIN_SRC c
mesh->cell = NULL;
   #+END_SRC
** Run the program

   Then I run the program with a reduce number of iteration (16) and without
   *MPI*. The program finish without crashing.

   I decide to run it with *MPI* but it doesn't give back again and doesn't
   print on standart output anything. I decide to inspect the code and find an
   error of communication maybe.

   The problem is that on the *main* function, we autorize only the
   *RANK_MASTER* process to execute the function *close_file*. But in this
   function we have a *MPI* barrier with *MPI_COMM_WORLD* that wait all
   process. And the problem is that others threads haven't a *MPI* barrier. Thus
   the *RANK_MASTER* wait infinitely.

   To patch this, I remove the line of *MPI* barrier in *close_file* function.

** Valgrind

   I decide to run *valgrind* now, but it seems to be normal. We haven't
   *possbilby lost* or *suppressed* bytes. But we have *833 bytes definitely
   lost*.

   #+BEGIN_SRC json
==29527== HEAP SUMMARY:
==29527==     in use at exit: 83,271 bytes in 50 blocks
==29527==   total heap usage: 21,807 allocs, 21,757 frees, 33,251,263 bytes allocated
==29527== 
==29527== LEAK SUMMARY:
==29527==    definitely lost: 833 bytes in 12 blocks
==29527==    indirectly lost: 599 bytes in 20 blocks
==29527==      possibly lost: 0 bytes in 0 blocks
==29527==    still reachable: 81,839 bytes in 18 blocks
==29527==         suppressed: 0 bytes in 0 blocks
   #+END_SRC

   Therfore I will check the code to see where memory is not release. But
   everything seems ok, I zap this part.

* Original Code
** Result

   Fist of all, I want to execute the code and generate a *gif* to see if it
   working. This is the last frame (frame 4 of 4):

   #+CAPTION: Last frame of Origin Code
   #+NAME: fig:last_frame_of_origin_code
   #+ATTR_LATEX: :width 300px
   [[./simu_simple_LBM/origin_code_last_frame.png]]

   It is not like the image in the subject. So I will investigate the code and
   see what is wrong. But for the moment I will zap this part.

** GIF, Image and script

   Also I have only *4* frame when I run your script. And an error occur:

   #+BEGIN_SRC shell
gnuplot> splot "< ./display --gnuplot result.raw 4" u 1:2:4
                                                           ^
         line 0: All points x value undefined
   #+END_SRC

   But the srcipt generate the *gif*

** Checksum Script

   I make a script which call *display* binary with the 2 file and the number of
   frame in parameter to compare their checksum.

** Communication
*** Explaination

    Fistr of all I reduce the number of iteration from *1600* to *16* and the
    number of processus *MPI* from *512* to *4*. Because it was too long to
    test.

    I am workeing on a personnal project of *MPI profiler* and for a number of
    *4* process and the following configuration:

    #+BEGIN_SRC shell
=================== CONFIG ===================
iterations           = 10
width                = 800
height               = 160
obstacle_r           = 17.000000
obstacle_x           = 161.000000
obstacle_y           = 83.000000
reynolds             = 100.000000
reynolds             = 100.000000
inflow_max_velocity  = 0.100000
output_filename      = resultat.raw
write_interval       = 50
------------ Derived parameters --------------
kinetic_viscosity    = 0.034000
relax_parameter      = 1.661130
==============================================
    #+END_SRC

    I obtain that:

    #+BEGIN_SRC shell
===============================================================================
================================= MPI PROFILER ================================
===============================================================================
GLOBAL SUMMARY:
        388317 message send
        388317 message recv
        436 barrier passed

LOCAL SUMMARY (Process 0):
        64719 message send [ 1 ]
        64722 message recv [ 1 2 3 ]
        109 barrier passed

LOCAL SUMMARY (Process 1):
        129439 message send [ 0 2 ]
        129438 message recv [ 0 2 ]
        109 barrier passed

LOCAL SUMMARY (Process 2):
        129439 message send [ 0 1 3 ]
        129438 message recv [ 1 3 ]
        109 barrier passed

LOCAL SUMMARY (Process 3):
        64720 message send [ 0 2 ]
        64719 message recv [ 2 ]
        109 barrier passed

ERROR SUMMARY:
        No error
    #+END_SRC

    For the moment, it is just an interposition library that interpose
    *MPI_Send*, *MPI_Recv* and *MPI_Barrier*.

    We can see that we have a lot of communication, and a lot of barrier. We can
    see that also in the code bacause we loop on *MPI_Send*, *MPI_Recv* and
    *MPI_Barrier* call.

    For the communication, if we consider that all process send their info to
    master process (process 0) for print information. We can see that the
    communication for *4* MPI process are the following:

    - each process communicate with his neighbors

      
    The code of my *MPI Profiler* will be added to my github this week. For the
    moment we need to preload manually the library with *LD_PRELOAD*.

    https://github.com/Sholde

    Here with *8* processus to confirm the communication scheme:

    #+BEGIN_SRC shell
===============================================================================
================================= MPI PROFILER ================================
===============================================================================
GLOBAL SUMMARY:
        906073 message send
        906073 message recv
        872 barrier passed

LOCAL SUMMARY (Process 0):
        64719 message send [ 1 ]
        64726 message recv [ 1 2 3 4 5 6 7 ]
        109 barrier passed

LOCAL SUMMARY (Process 1):
        129439 message send [ 0 2 ]
        129438 message recv [ 0 2 ]
        109 barrier passed

LOCAL SUMMARY (Process 2):
        129439 message send [ 0 1 3 ]
        129438 message recv [ 1 3 ]
        109 barrier passed

LOCAL SUMMARY (Process 3):
        129439 message send [ 0 2 4 ]
        129438 message recv [ 2 4 ]
        109 barrier passed

LOCAL SUMMARY (Process 4):
        129439 message send [ 0 3 5 ]
        129438 message recv [ 3 5 ]
        109 barrier passed

LOCAL SUMMARY (Process 5):
        129439 message send [ 0 4 6 ]
        129438 message recv [ 4 6 ]
        109 barrier passed

LOCAL SUMMARY (Process 6):
        129439 message send [ 0 5 7 ]
        129438 message recv [ 5 7 ]
        109 barrier passed

LOCAL SUMMARY (Process 7):
        64720 message send [ 0 6 ]
        64719 message recv [ 6 ]
        109 barrier passed

ERROR SUMMARY:
        No error
        #+END_SRC

        So it is not the scheme describe in the subject were we have a cube, and
        we can exchange in diagonally, and vertically. Here we exchange only
        horizontally.

        But on the code, we exchange vetically and diagonally, so I don't know
        why we don't exchange. Maybe the initialisation was not correct.

*** Scheme
   
    #+CAPTION: Scheme of Original Code
    #+NAME: fig:scheme_of_original_code
    #+ATTR_LATEX: :width 300px
    [[./ressources/scheme_of_original_code.png]]
** Scalability of Original Code

   I added few lines to compute the times of the code with *MPI_Wtime*. And I
   print only the time of rank master. I choose to compute the time of the loop
   in *main.c* because is here we work.

   There are *2 ways* to determine the scability:
   - *strong scaling:* when we increase the number of processus by *2*, we reduce
     the time by *2*
   - *weak scaling:* when we increase the quantity of data by *2* and the number of
     processus by *2*, the time is the same

*** Strong scaling

    #+BEGIN_SRC sh
# 1 process
Elapsed time in second(s): 9.617342     

# 2 process
Elapsed time in second(s): 9.463310

# 4 process
Elapsed time in second(s): 9.271398
    #+END_SRC

    We can see when we increase the number of processus by *2*, we don't reduce
    the time by *2*. So we don't scale.

*** Weak scaling

    To evaluate the *weak scaling* I decide to begin with default config:

    #+BEGIN_SRC json
iterations           = 10
width                = 800
height               = 160
#obstacle_r           = 
#obstacle_x           = 
#obstacle_y           = 
reynolds             = 100
inflow_max_velocity  = 0.100000
inflow_max_velocity  = 0.100000
output_filename      = resultat.raw
write_interval       = 50
    #+END_SRC

    And I will multiply the height by *2* when I increase the number of
    processus by *2*, because if I do that I multiply the quantity of data by
    *2*, if I multiply also the width by *2*, then I multiply the quantity of
    data by *4* and it is not that we want.

    So for *1 process* height equal *160*, for *2 process* height equal *320*
    and for *4 process* height equal *740*.

    #+BEGIN_SRC sh
# 1 process
Elapsed time in second(s): 9.764508

# 2 process
Elapsed time in second(s): 9.662370

# 4 process
Elapsed time in second(s): 9.840443
    #+END_SRC

    We scale because when we increase the number of processus by 2 and the
    quantity of data by 2, we take the same time.
* Optimization
** Introduction

   I run the original code with the config file *original_config.txt* (with 160
   iteration to have *4* frame and be able to compare the checksum when I will
   optimize the code). The result is on *original_code.raw*.

   And I run with *4* process *MPI* (because I have only *4* process and *512*
   is not accept by *MPI* when I *oversubscribe*).

   The time for original code is: *165.417102 seconds*.

** Barrier MPI
*** Identification

    Like you can see above in the report, my *MPI profile* detect a lot of
    *barrier*.

*** Justification

    But in another course, we said us that *MPI_Barrier* are useless
    because we are on a distributed memory (not shared). But only for print we
    can keep them, for keep an readable output.

    I don't need other justification to delete all of them (almost).

*** Evaluation

    I run the code with the same config but the time stay the same, it is *165*
    seconds (~2 min 45).

    I check the checksum with my script and it is the same so it don't change
    the behaviour.
    
    #+CAPTION: Without MPI Barrier
    #+NAME: fig:without_barrier
    #+ATTR_LATEX: :width 300px
    [[./simu_simple_LBM/without_barrier.png]]

    I decide to not evaluate the scability because it is not barrier that cause
    the *slow-down*. (normally it is the main problem but not here)

    Therefore this optimization doesn't affect the code for the moment.

